{"cells":[{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2023-12-08T15:58:04.683010Z","iopub.status.busy":"2023-12-08T15:58:04.682412Z","iopub.status.idle":"2023-12-08T15:58:04.687943Z","shell.execute_reply":"2023-12-08T15:58:04.686875Z","shell.execute_reply.started":"2023-12-08T15:58:04.682980Z"},"id":"oH0Lv6qP4qQu","trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import cv2\n","import torch.nn.functional as F\n","import torchvision.transforms.functional as TF\n","from torch.cuda.amp import GradScaler"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2023-12-08T15:58:04.741226Z","iopub.status.busy":"2023-12-08T15:58:04.740961Z","iopub.status.idle":"2023-12-08T15:58:04.757469Z","shell.execute_reply":"2023-12-08T15:58:04.756461Z","shell.execute_reply.started":"2023-12-08T15:58:04.741203Z"},"id":"QR8-za1_5CfQ","trusted":true},"outputs":[],"source":["# def crop_image(tensor, target):\n","#     tensor_size = tensor.size()[2]\n","#     target_size = target.size()[2]\n","#     delta = tensor_size - target_size\n","#     delta = delta // 2\n","#     return tensor[:, :, delta:tensor_size-delta, delta:tensor_size-delta]\n","\n","def crop_image(tensor, target):\n","    if tensor.shape != target.shape:\n","        tensor = TF.resize(tensor, size=target.shape[2:])\n","    return tensor\n","\n","def double_conv(in_ch, out_ch):\n","    conv = nn.Sequential(\n","        nn.Conv2d(in_ch, out_ch, kernel_size=3, stride=1, padding=1),\n","        nn.BatchNorm2d(out_ch),\n","        nn.ReLU(inplace=True),\n","        nn.Conv2d(out_ch, out_ch, kernel_size=3, stride=1, padding=1),\n","        nn.BatchNorm2d(out_ch),\n","        nn.ReLU(inplace=True)\n","    )\n","    return conv\n","\n","class UNET(nn.Module):\n","    def __init__(self):\n","        super(UNET, self).__init__()\n","\n","        self.max_pool_2x2 = nn.MaxPool2d(kernel_size=2, stride=2)\n","\n","        self.double_conv_1 = double_conv(3, 64)\n","        self.double_conv_2 = double_conv(64, 128)\n","        self.double_conv_3 = double_conv(128, 256)\n","        self.double_conv_4 = double_conv(256, 512)\n","        self.double_conv_5 = double_conv(512, 1024)\n","\n","        self.up_trans_1 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n","        self.up_conv_1 = double_conv(1024, 512)\n","\n","        self.up_trans_2 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n","        self.up_conv_2 = double_conv(512, 256)\n","\n","        self.up_trans_3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n","        self.up_conv_3 = double_conv(256, 128)\n","\n","        self.up_trans_4 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n","        self.up_conv_4 = double_conv(128, 64)\n","\n","        self.final_conv = nn.Conv2d(64, 1, kernel_size=1)\n","\n","    def forward(self, image):\n","        #DownSample\n","        x1 = self.double_conv_1(image)\n","        x2 = self.max_pool_2x2(x1)\n","        x3 = self.double_conv_2(x2)\n","        x4 = self.max_pool_2x2(x3)\n","        x5 = self.double_conv_3(x4)\n","        x6 = self.max_pool_2x2(x5)\n","        x7 = self.double_conv_4(x6)\n","        x8 = self.max_pool_2x2(x7)\n","        x9 = self.double_conv_5(x8)\n","        \n","        #Bottleneck \n","        x = self.up_trans_1(x9)\n","        y = crop_image(x7, x)\n","        x = self.up_conv_1(torch.cat([x, y], dim=1))\n","        \n","        #UpSample\n","        x = self.up_trans_2(x)\n","        y = crop_image(x5, x)          \n","#         print(f\"{y.shape} -- {x.shape}\")\n","        x = self.up_conv_2(torch.cat([x, y], dim=1))\n","\n","        x = self.up_trans_3(x)\n","        y = crop_image(x3, x)        \n","#         print(f\"{y.shape} -- {x.shape}\")\n","        x = self.up_conv_3(torch.cat([x, y], dim=1))\n","\n","        x = self.up_trans_4(x)        \n","        y = crop_image(x1, x)\n","#         print(f\"{y.shape} -- {x.shape}\")\n","        x = self.up_conv_4(torch.cat([x, y], dim=1))\n","        \n","#         print(f\"{x.shape}\")\n","\n","        x = self.final_conv(x)\n","        \n","        return x\n"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2023-12-08T15:58:04.837294Z","iopub.status.busy":"2023-12-08T15:58:04.837036Z","iopub.status.idle":"2023-12-08T15:58:04.846693Z","shell.execute_reply":"2023-12-08T15:58:04.845811Z","shell.execute_reply.started":"2023-12-08T15:58:04.837272Z"},"id":"y8PL8TA4YEQd","trusted":true},"outputs":[],"source":["import os\n","from PIL import Image\n","from torch.utils.data import Dataset\n","import numpy as np\n","\n","class WatermarkDataset(Dataset):\n","    def __init__(self, image_dir, mask_dir, transform=None):\n","        self.image_dir = image_dir\n","        self.mask_dir = mask_dir\n","        self.transform = transform\n","        self.images = os.listdir(image_dir)\n","\n","    def __len__(self):\n","        return len(self.images)\n","\n","    # Make some changes here\n","    def __getitem__(self, index):\n","        img_path = os.path.join(self.image_dir, self.images[index])\n","        mask_path = os.path.join(self.mask_dir, self.images[index].replace('train2014', 'train2014_mask'))\n","        image = np.array(Image.open(img_path).convert(\"RGB\"))\n","        mask = np.array(Image.open(mask_path).convert(\"L\"), dtype=np.float32)\n","        mask[mask == 255.0] = 1.0\n","\n","        # Ensure mask has the same dimensions as the image\n","        if image.shape[:2] != mask.shape:\n","            mask = cv2.resize(mask, (image.shape[1], image.shape[0]))\n","\n","        if self.transform is not None:\n","            augmentations = self.transform(image=image, mask=mask)\n","            image = augmentations['image']\n","            mask = augmentations['mask']\n","\n","        return image, mask\n","\n","# Done\n"]},{"cell_type":"code","execution_count":35,"metadata":{"execution":{"iopub.execute_input":"2023-12-08T16:20:46.709167Z","iopub.status.busy":"2023-12-08T16:20:46.708812Z","iopub.status.idle":"2023-12-08T16:20:46.726801Z","shell.execute_reply":"2023-12-08T16:20:46.725635Z","shell.execute_reply.started":"2023-12-08T16:20:46.709131Z"},"id":"_hH3sgCAUXYr","trusted":true},"outputs":[],"source":["import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","import torch.optim as optim\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","\n","LEARNING_RATE = 1e-5\n","DEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\n","BATCH_SIZE = 8\n","NUM_EPOCHS = 50\n","NUM_WORKERS = 2\n","IMAGE_HEIGHT = 572\n","IMAGE_WIDTH = 572\n","LOAD_MODEL = True \n","TRAIN_IMG_DIR = '/kaggle/input/watermark-extraction/Datasets/images'\n","TRAIN_MASK_DIR = '/kaggle/input/watermark-extraction/Datasets/watermarks'\n","VAL_IMG_DIR = '/kaggle/input/watermark-extraction/Datasets/val_images'\n","VAL_MASK_DIR = '/kaggle/input/watermark-extraction/Datasets/val_watermarks'\n","\n","def train_fn(loader, model, optimizer, loss_fn, scaler):\n","    loop = tqdm(loader)\n","    for batch_idx, (data, targets) in enumerate(loop):\n","        data = data.to(device=DEVICE)\n","        targets = targets.float().unsqueeze(1).to(device=DEVICE)\n","\n","        # forward\n","        with torch.cuda.amp.autocast():\n","            predictions = model(data)\n","            predictions = F.interpolate(\n","                predictions, size=(572,572), mode='bilinear', align_corners=True\n","            )\n","#             print(f'{predictions.size()} -- {targets.size()}')\n","            loss = loss_fn(predictions, targets)\n","\n","        # backward\n","        optimizer.zero_grad()\n","        scaler.scale(loss).backward()\n","        scaler.step(optimizer)\n","        scaler.update()\n","\n","        # update tqdm loop\n","        loop.set_postfix(loss=loss.item())\n","    return loss\n","\n","def main():\n","    train_transform = A.Compose(\n","        [\n","            A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n","            A.Rotate(limit=35, p=1.0),\n","            A.HorizontalFlip(p=0.5),\n","            A.VerticalFlip(p=0.5),\n","            A.Normalize(\n","                mean=[0.0, 0.0, 0.0],\n","                std=[1.0, 1.0, 1.0],\n","                max_pixel_value=255.0,\n","            ),\n","            ToTensorV2(),\n","        ],\n","    )\n","\n","    val_transform = A.Compose(\n","        [\n","            A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n","            A.Normalize(\n","                mean=[0.0, 0.0, 0.0],\n","                std=[1.0, 1.0, 1.0],\n","                max_pixel_value=255.0,\n","            ),\n","            ToTensorV2(),\n","        ],\n","    )\n","\n","    model = UNET().to(DEVICE)\n","    loss_fn = nn.BCEWithLogitsLoss()\n","    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n","\n","    train_loader, val_loader = get_loaders(\n","        TRAIN_IMG_DIR,\n","        TRAIN_MASK_DIR,\n","        VAL_IMG_DIR,\n","        VAL_MASK_DIR,\n","        BATCH_SIZE,\n","        train_transform,\n","        val_transform,\n","        NUM_WORKERS,\n","    )\n","\n","    if LOAD_MODEL:\n","        load_checkpoint(torch.load(\"my_checkpoint.pth.tar\"), model)\n","\n","    check_accuracy(val_loader, model, device=DEVICE)\n","    scaler = GradScaler()\n","\n","    for epoch in range(NUM_EPOCHS):\n","        train_loss_values = train_fn(train_loader, model, optimizer, loss_fn, scaler)\n","\n","        # save model\n","        checkpoint = {\n","            'state_dict': model.state_dict(),\n","            'optimizer': optimizer.state_dict(),\n","        }\n","        save_checkpoint(checkpoint)\n","\n","        # check accuracy\n","        check_accuracy(val_loader, model, device=DEVICE)\n","\n","        # print some examples to the folder\n","        save_predictions_as_imgs(\n","            val_loader, model, folder='saved_images/', device=DEVICE\n","        )\n","\n","\n"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2023-12-08T15:58:04.986285Z","iopub.status.busy":"2023-12-08T15:58:04.986028Z","iopub.status.idle":"2023-12-08T15:58:05.002834Z","shell.execute_reply":"2023-12-08T15:58:05.002123Z","shell.execute_reply.started":"2023-12-08T15:58:04.986261Z"},"id":"g4uOqnt-d_F9","trusted":true},"outputs":[],"source":["import torchvision\n","from torch.utils.data import DataLoader\n","\n","def save_checkpoint(state, filename = 'my_checkpoint.pth.tar'):\n","    print(\"===> Saving the checkpoint....\")\n","    torch.save(state, filename)\n","\n","def load_checkpoint(checkpoint, model):\n","    print(\"===> Loading the checkpoint....\")\n","    model.load_state_dict(checkpoint['state_dict'])\n","\n","def get_loaders(\n","    train_img_dir,\n","    train_mask_dir,\n","    val_img_dir,\n","    val_mask_dir,\n","    batch_size,\n","    train_transform,\n","    val_transform,\n","    num_workers=4,\n","):\n","\n","    train_ds = WatermarkDataset(\n","      image_dir = train_img_dir,\n","      mask_dir = train_mask_dir,\n","      transform = train_transform\n","  )\n","\n","    train_loader = DataLoader(\n","      train_ds,\n","      batch_size = batch_size,\n","      num_workers = num_workers,\n","      shuffle = True\n","  )\n","\n","    val_ds = WatermarkDataset(\n","      image_dir = val_img_dir,\n","      mask_dir = val_mask_dir,\n","      transform = val_transform\n","  )\n","\n","    val_loader = DataLoader(\n","      val_ds,\n","      batch_size = batch_size,\n","      num_workers = num_workers,\n","      shuffle = False\n","  )\n","\n","    return train_loader, val_loader\n","\n","def check_accuracy(loader, model, device=DEVICE):\n","    num_correct = 0\n","    num_pixels =  0\n","    dice_score = 0\n","#     n = 0\n","    model.eval()\n","\n","    with torch.no_grad():\n","        for x, y in loader:\n","            x = x.to(device)\n","            y = y.to(device).unsqueeze(1)\n","            preds = torch.sigmoid(model(x))\n","            preds = (preds > 0.5).float()\n","            preds = F.interpolate(preds, size=(572,572), mode='bilinear', align_corners=True)\n","            num_correct += (preds == y).sum()\n","            num_pixels += torch.numel(y)\n","            intersection = torch.sum(preds * y)\n","            dice_score += (2.0 * intersection) / (torch.sum(preds) + torch.sum(y) + 1e-8)\n","#             print(len(x))\n","#             n += 1\n","\n","    print(\n","        f\"Got {num_correct}/{num_pixels} with acc {(num_correct/num_pixels)*100:.2f}\"\n","        )\n","#     print(f\"Dice : {dice_score.item()}\")\n","    \n","#     print(f\"Samples : {loader.dataset.scalar_value}\")\n","#     print(f\"{n} \")\n","    print(f\"Dice Score : {dice_score.item()/95}\")\n","    model.train()\n","    return (num_correct)/(num_pixels)\n","\n"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2023-12-08T15:58:05.004118Z","iopub.status.busy":"2023-12-08T15:58:05.003836Z","iopub.status.idle":"2023-12-08T15:58:05.019351Z","shell.execute_reply":"2023-12-08T15:58:05.018345Z","shell.execute_reply.started":"2023-12-08T15:58:05.004095Z"},"trusted":true},"outputs":[],"source":["import os\n","import torch\n","import torchvision\n","from torchvision.utils import save_image\n","\n","def save_predictions_as_imgs(loader, model, folder='saved_images/', device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')):\n","    model.eval()\n","\n","    # Create the folder if it doesn't exist\n","    if not os.path.exists(folder):\n","        os.makedirs(folder)\n","\n","    for idx, (x, y) in enumerate(loader):\n","        x = x.to(device)\n","        with torch.no_grad():\n","            preds = torch.sigmoid(model(x))\n","            preds = (preds > 0.5).float()\n","        save_image(\n","            preds, f\"{folder}/pred_{idx}.png\"\n","        )\n","        save_image(y.unsqueeze(1), f\"{folder}/gt_{idx}.png\")\n","\n","    model.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-08T16:22:36.029374Z","iopub.status.busy":"2023-12-08T16:22:36.028685Z","iopub.status.idle":"2023-12-08T17:29:14.237751Z","shell.execute_reply":"2023-12-08T17:29:14.236435Z","shell.execute_reply.started":"2023-12-08T16:22:36.029346Z"},"id":"pkFVXeuXfkr8","outputId":"7b6299a5-5916-4f47-ed06-f6e4c72a9fd6","trusted":true},"outputs":[],"source":["main()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-08T16:22:27.826523Z","iopub.status.busy":"2023-12-08T16:22:27.826012Z","iopub.status.idle":"2023-12-08T16:22:27.834302Z","shell.execute_reply":"2023-12-08T16:22:27.833343Z","shell.execute_reply.started":"2023-12-08T16:22:27.826482Z"},"trusted":true},"outputs":[],"source":["import shutil\n","\n","# Assuming 'source_file' is the path to your file and 'destination' is where you want to save it\n","if not os.path.exists('/kaggle/working/output_folder/'):\n","        os.makedirs('/kaggle/working/output_folder/')\n","source_file = '/kaggle/working/output_folder/my_checkpoint.pth.tar'\n","destination = '/kaggle/working/my_checkpoint.pth.tar'\n","\n","shutil.move(source_file, destination)\n"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"toc_visible":true},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":1843391,"sourceId":3009448,"sourceType":"datasetVersion"},{"datasetId":4086304,"sourceId":7091364,"sourceType":"datasetVersion"},{"datasetId":4086625,"sourceId":7091550,"sourceType":"datasetVersion"}],"dockerImageVersionId":30588,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
